{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from efficient_reasoning.mcts import Node\n",
    "from efficient_reasoning.utils import last_boxed_only_string, remove_boxed, is_equiv, AutoScoringJudge\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from typing import TypeAlias, Literal, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Benchmark: TypeAlias = Literal[\"AIME_2024\", \"MATH-500\", \"OlympiadBench-674-MATH_TO_EN\"]\n",
    "dataset = 'MATH-500'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'../data/{dataset}/train.jsonl'\n",
    "datapoints = []\n",
    "with open(path, 'r') as f:\n",
    "    for line in f:\n",
    "        datapoints.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 10:23:40 config.py:526] This model supports multiple tasks: {'generate', 'score', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 02-25 10:23:41 config.py:1383] Defaulting to use mp for distributed inference\n",
      "INFO 02-25 10:23:41 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='Qwen/Qwen2.5-3B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 02-25 10:23:41 multiproc_worker_utils.py:298] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-25 10:23:41 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:23:41 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m INFO 02-25 10:23:41 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:23:41 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "INFO 02-25 10:23:42 cuda.py:235] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:23:42 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 02-25 10:23:42 cuda.py:235] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m INFO 02-25 10:23:43 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 02-25 10:23:44 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 02-25 10:23:44 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:23:44 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 02-25 10:23:44 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 02-25 10:23:44 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m INFO 02-25 10:23:44 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:23:44 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:23:44 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "WARNING 02-25 10:23:44 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m WARNING 02-25 10:23:44 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 02-25 10:23:44 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 02-25 10:23:44 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 02-25 10:23:45 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_022e81a1'), local_subscribe_port=38375, remote_subscribe_port=None)\n",
      "INFO 02-25 10:23:45 model_runner.py:1111] Starting to load model Qwen/Qwen2.5-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m INFO 02-25 10:23:45 model_runner.py:1111] Starting to load model Qwen/Qwen2.5-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:23:45 model_runner.py:1111] Starting to load model Qwen/Qwen2.5-3B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:23:45 model_runner.py:1111] Starting to load model Qwen/Qwen2.5-3B-Instruct...\n",
      "INFO 02-25 10:23:45 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:23:45 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:23:45 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 10:23:45 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.44it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.77it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.70it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 10:23:46 model_runner.py:1116] Loading model weights took 1.4820 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:23:46 model_runner.py:1116] Loading model weights took 1.4820 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:23:46 model_runner.py:1116] Loading model weights took 1.4820 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m INFO 02-25 10:23:46 model_runner.py:1116] Loading model weights took 1.4820 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m INFO 02-25 10:23:51 worker.py:266] Memory profiling takes 4.72 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m INFO 02-25 10:23:51 worker.py:266] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.90) = 42.78GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m INFO 02-25 10:23:51 worker.py:266] model weights take 1.48GiB; non_torch_memory takes 0.65GiB; PyTorch activation peak memory takes 1.14GiB; the rest of the memory reserved for KV Cache is 39.51GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:23:51 worker.py:266] Memory profiling takes 4.79 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:23:51 worker.py:266] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.90) = 42.78GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:23:51 worker.py:266] model weights take 1.48GiB; non_torch_memory takes 0.65GiB; PyTorch activation peak memory takes 1.14GiB; the rest of the memory reserved for KV Cache is 39.51GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:23:51 worker.py:266] Memory profiling takes 4.87 seconds\n",
      "INFO 02-25 10:23:51 worker.py:266] Memory profiling takes 4.88 seconds\n",
      "INFO 02-25 10:23:51 worker.py:266] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.90) = 42.78GiB\n",
      "INFO 02-25 10:23:51 worker.py:266] model weights take 1.48GiB; non_torch_memory takes 0.71GiB; PyTorch activation peak memory takes 1.51GiB; the rest of the memory reserved for KV Cache is 39.08GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:23:51 worker.py:266] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.90) = 42.78GiB\n",
      "INFO 02-25 10:23:51 executor_base.py:108] # CUDA blocks: 142278, # CPU blocks: 14563\n",
      "INFO 02-25 10:23:51 executor_base.py:113] Maximum concurrency for 32768 tokens per request: 69.47x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:23:51 worker.py:266] model weights take 1.48GiB; non_torch_memory takes 0.65GiB; PyTorch activation peak memory takes 1.14GiB; the rest of the memory reserved for KV Cache is 39.51GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:23:54 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:23:54 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m INFO 02-25 10:23:54 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-25 10:23:54 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 10:24:14 model_runner.py:1563] Graph capturing finished in 20 secs, took 0.46 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582467)\u001b[0;0m INFO 02-25 10:24:14 model_runner.py:1563] Graph capturing finished in 20 secs, took 0.46 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582462)\u001b[0;0m INFO 02-25 10:24:14 model_runner.py:1563] Graph capturing finished in 20 secs, took 0.46 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3582472)\u001b[0;0m INFO 02-25 10:24:14 model_runner.py:1563] Graph capturing finished in 20 secs, took 0.46 GiB\n",
      "INFO 02-25 10:24:14 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 28.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "end_of_text_token = \"<|end_of_text|>\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "llm = LLM(model=model, \n",
    "          tensor_parallel_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    n=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(response_text_list: List[str], verbose: bool = False) -> Tuple[List[str], List[str]]:\n",
    "    # # get the last 4 lines of the response text\n",
    "    last_four_lines_list = []\n",
    "    for response_text in response_text_list:\n",
    "        response_text = response_text.strip()\n",
    "        last_four_lines = \"\".join(response_text.split(\"\\n\")[-4:])\n",
    "        last_four_lines_list.append(last_four_lines)\n",
    "\n",
    "    # returning `failed_last_line_list` for debugging purposes\n",
    "    final_answer_list = []\n",
    "    failed_list = []\n",
    "\n",
    "    for last_four_lines in last_four_lines_list:\n",
    "        # extract final answer with latex box: \\boxed{}, \\fbox{}, \\framebox{}, \\x08oxed{}\n",
    "        boxed_answer = last_boxed_only_string(last_four_lines)\n",
    "        # if no boxed answer is found, use an error message as the placeholder for the final answer\n",
    "        if not boxed_answer:\n",
    "            if verbose:\n",
    "                print(f\"Error: no boxed answer found in the last four lines: {last_four_lines}\")\n",
    "            final_answer_list.append(\"Error: no boxed answer found\")\n",
    "            failed_list.append(last_four_lines)\n",
    "            continue\n",
    "        # if the boxed answer is found, remove the latex box\n",
    "        else:\n",
    "            final_answer = remove_boxed(boxed_answer)\n",
    "            final_answer_list.append(final_answer)\n",
    "\n",
    "    return final_answer_list, failed_list\n",
    "\n",
    "def compute_accuracy(\n",
    "    benchmark: Benchmark, ground_truth_list: List[str], final_answer_list: List[str], verbose: bool = False\n",
    ") -> List[bool]:\n",
    "    # check if the number of final answers and ground truths are equal\n",
    "    assert len(final_answer_list) == len(ground_truth_list), \"The number of final answers and ground truths should be equal.\"\n",
    "\n",
    "    # initialize the scorer from OlympiadBench, it's almost compatible with the AIME_2024 and MATH benchmarks\n",
    "    # excpet for cases like \\$18.90 and 18.90, which can be handled by `is_equiv` but not `AutoScoringJudge`\n",
    "    # in general, the `AutoScoringJudge` is more robust and can handle more cases, see test cases in `evaluation.py`\n",
    "    scorer = AutoScoringJudge()\n",
    "    accuracy_result_list = []\n",
    "\n",
    "    # use the corresponding accuracy metric for the benchmark\n",
    "    for index, line in enumerate(final_answer_list):\n",
    "        ground_truth = ground_truth_list[index]\n",
    "        final_answer = final_answer_list[index]\n",
    "\n",
    "        # if failed to extract the final answer, set the accuracy to False\n",
    "        if final_answer == \"Error: no boxed answer found\":\n",
    "            accuracy_result = False\n",
    "        # for AIME 2024, `AutoScoringJudge` is completely compatible\n",
    "        elif benchmark == \"AIME_2024\":\n",
    "            accuracy_result = scorer.judge(ground_truth, final_answer)\n",
    "        # for MATH, use both `is_equiv` and `AutoScoringJudge` for more robust equivalence checking\n",
    "        elif benchmark == \"MATH-500\":\n",
    "            accuracy_result = is_equiv(ground_truth, final_answer) or scorer.judge(ground_truth, final_answer)\n",
    "        # for OlympiadBench, use the native `AutoScoringJudge`\n",
    "        elif benchmark == \"OlympiadBench-674-MATH_TO_EN\":\n",
    "            ground_truth_answer, precision = ground_truth\n",
    "            if not precision:\n",
    "                accuracy_result = scorer.judge(ground_truth_answer, final_answer)\n",
    "            else:\n",
    "                accuracy_result = scorer.judge(ground_truth_answer, final_answer, precision=float(precision))\n",
    "        # other benchmarks are not supported\n",
    "        else:\n",
    "            raise ValueError(f\"Benchmark: {benchmark} is not supported.\")\n",
    "\n",
    "        # if `verbose` is set True, print the final answer and ground truth\n",
    "        if verbose:\n",
    "            print(f\"Ground Truth: {ground_truth}, Final Answer: {final_answer}, Accuracy: {accuracy_result}\")\n",
    "\n",
    "        accuracy_result_list.append(accuracy_result)\n",
    "\n",
    "    return accuracy_result_list\n",
    "\n",
    "\n",
    "def evaluate(benchmark, responses, ground_truth_list, verbose=False):\n",
    "\n",
    "    # construct the final answer list\n",
    "    final_answer_list, failed_list = extract_final_answer(responses, verbose)\n",
    "\n",
    "    # compute the accuracy result list\n",
    "    accuracy_result_list = compute_accuracy(benchmark, ground_truth_list, final_answer_list, verbose)\n",
    "    \n",
    "    return accuracy_result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vine(Node):\n",
    "    def __init__(self, demonstration_steps: List[str], llm: LLM, sampling_params: SamplingParams, curr_step_index: int, target: str, value: float, benchmark: Benchmark):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "        self.llm = llm\n",
    "        self.sampling_params = sampling_params\n",
    "        self.curr_step_index = curr_step_index\n",
    "        self.target = target\n",
    "        self.demonstration_steps = demonstration_steps\n",
    "        self.benchmark = benchmark\n",
    "        self.roll_out()\n",
    "        \n",
    "    def roll_out(self):\n",
    "        \n",
    "        # generate responses\n",
    "        self.responses = self.llm.generate(\n",
    "            \"\".join(self.demonstration_steps[:self.curr_step_index+1]),\n",
    "            sampling_params=self.sampling_params,\n",
    "        )\n",
    "        \n",
    "        self.responses = [response.text for response in self.responses[0].outputs]\n",
    "        \n",
    "        # evaluate the responses\n",
    "        rewards = evaluate(self.benchmark, self.responses, [self.target]*len(self.responses))\n",
    "        \n",
    "        # compute average reward (Q-value)\n",
    "        self.q_value = np.mean(rewards)\n",
    "        \n",
    "        # compute the advantage\n",
    "        self.advantage = self.q_value - self.value\n",
    "    \n",
    "    def find_children(self):\n",
    "        if self.curr_step_index == len(self.demonstration_steps) - 1:\n",
    "            return []\n",
    "        \n",
    "        return [\n",
    "            Vine(\n",
    "                demonstration_steps=self.demonstration_steps,\n",
    "                llm=self.llm,\n",
    "                sampling_params=self.sampling_params,\n",
    "                curr_step_index=self.curr_step_index + 1,\n",
    "                target=self.target,\n",
    "                value=self.q_value,\n",
    "                benchmark=self.benchmark\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def find_random_child(self):\n",
    "        return self.find_children()[0]\n",
    "    \n",
    "    def reward(self):\n",
    "        return evaluate(self.benchmark, self.responses, [self.target]*len(self.responses))\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return self.curr_step_index == len(self.demonstration_steps) - 1\n",
    "    \n",
    "    def make_move(self):\n",
    "        return self.find_children()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.split(\"/\")[-1]\n",
    "step_limit = 10\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [00:03<00:03,  3.46s/it, est. speed input: 7.23 toks/s, output: 194.43 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n",
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.21it/s, est. speed input: 108.45 toks/s, output: 214.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n",
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  1.08it/s, est. speed input: 53.16 toks/s, output: 218.04 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n",
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.87it/s, est. speed input: 245.79 toks/s, output: 213.94 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n",
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  2.83it/s, est. speed input: 241.63 toks/s, output: 170.55 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n",
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  9.41it/s, est. speed input: 943.56 toks/s, output: 133.41 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n",
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  9.65it/s, est. speed input: 967.71 toks/s, output: 136.80 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n",
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  8.94it/s, est. speed input: 901.15 toks/s, output: 127.40 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n",
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 1/2 [00:00<00:00,  9.40it/s, est. speed input: 941.18 toks/s, output: 133.07 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n",
      "ANTLR runtime and generated code versions disagree: 4.13.2!=4.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark = \"MATH-500\"\n",
    "games = []\n",
    "for i, problem in enumerate(datapoints):\n",
    "    target = problem[\"answer\"]\n",
    "    demonstration_steps = [problem['problem']] + problem[\"solution\"].split(\".\")\n",
    "    demonstration_tokens = []\n",
    "    for step in demonstration_steps:\n",
    "        demonstration_tokens.extend(tokenizer.encode(step))\n",
    "    curr_step_index = 0\n",
    "    game = {}\n",
    "    game['problem'] = problem\n",
    "    game['index'] = i\n",
    "    game['demonstration_steps'] = demonstration_steps\n",
    "    game['advantage'] = []\n",
    "    game['q_value'] = []\n",
    "    game['value'] = []\n",
    "    while True:\n",
    "        vinegame = Vine(\n",
    "            demonstration_steps=demonstration_steps, \n",
    "            llm=llm, \n",
    "            sampling_params=sampling_params, \n",
    "            curr_step_index=curr_step_index, \n",
    "            target=target, \n",
    "            value=0, \n",
    "            benchmark=benchmark)\n",
    "        curr_step_index += 1\n",
    "        game['advantage'].append(vinegame.advantage)\n",
    "        game['q_value'].append(vinegame.q_value)\n",
    "        game['value'].append(vinegame.value)\n",
    "        \n",
    "        if curr_step_index == len(demonstration_steps):\n",
    "            break\n",
    "        else:\n",
    "            vinegame = vinegame.find_children()[0]\n",
    "    breakpoint()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
